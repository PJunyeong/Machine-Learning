# -*- coding: utf-8 -*-
"""ML_2016130927

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XRYyH14L3CZRDPaSl2wgQI9KUaB8yt12

# ğŸ“§ ìŠ¤íŒ¸ í•„í„°ë§: ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ê¸°ë°˜ LSTM í•„í„°ë§ ëª¨ë¸

1.   ë°ì´í„° ì‹œê°í™”

2.   ë°ì´í„° ì „ì²˜ë¦¬

    *   ë°ì´í„° í´ë¦¬ë‹
    *   Countvectorizer, TF-IDF
    *   Tokenizer

3.   ML ëª¨ë¸ ë° ì„±ëŠ¥ í‰ê°€
    *   MultinomialNB
    *   Logistic Regression
    *   LSTM

4.   ìŠ¤íŒ¸ í•„í„°ë§ ëª¨ë¸
    *   ëª¨ë¸ ì„ ì •: í•„í„°ë§ ê¸°ë°˜ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë° LSTM
    *   í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
"""

!pip install contractions

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import collections
import contractions
import re
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report,  plot_confusion_matrix

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('wordnet')

import keras
from keras.layers import Dense, Embedding, LSTM, Dropout
from keras.models import Sequential
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

# train_set = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/train.csv')
# test_set = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/test.csv')
# Colabìš© íŒŒì¼ ê²½ë¡œ

train_set = pd.read_csv('./data/train.csv')
test_set = pd.read_csv('./data/test.csv'')
# ì œì¶œìš© íŒŒì¼ ê²½ë¡œ

train_set.info()
# train_setì— null ìˆëŠ”ì§€ í™•ì¸í•œë‹¤.

train_set = train_set.drop_duplicates(['mail'])
train_set.info()
# (3620-3528=92)ê°œ ì¤‘ë³µ ë°ì´í„° ì‚­ì œ

"""# 1. ë°ì´í„° ì‹œê°í™”
ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë‹¨ì–´ ë¹ˆë„ì— ë”°ë¼ ì‹œê°í™”í•œë‹¤.
"""

train_set['label'].value_counts().plot(kind = 'bar', title="HAM : SPAM", ylabel='count', xlabel='label')

"""ì£¼ì–´ì§„ ë°ì´í„° ì¤‘ ìŠ¤íŒ¸ì´ ë” ì ë‹¤(2500ê°œ ëŒ€ 1000ê°œ ì •ë„)."""

train_set['length'] = train_set['mail'].apply(len)

plt.figure(figsize=(12,18))
plt.hist(train_set['length'], bins = 100, alpha=0.3, color = 'r')
plt.hist(train_set[train_set['label']==1]['length'], bins = 100, alpha=0.3, color = 'g')
plt.hist(train_set[train_set['label']==0]['length'], bins = 100, alpha=0.3, color = 'b')
plt.text(3000, 1000, "max length in train set's mail: " + str(max(train_set['length'])), fontsize=20, color='r')
plt.text(3000, 960, "avg length in train set's mail:" + str(sum(train_set['length'])/len(train_set['length'])), fontsize=20, color='r')
plt.text(3000, 920, "max length in train set's spam: " + str(max(train_set[train_set['label']==1]['length'])), fontsize=20, color='g')
plt.text(3000, 880, "avg length in train set's spam:" + str(sum(train_set[train_set['label']==1]['length']) / len(train_set[train_set['label']==1]['length'])), fontsize=20, color='g')
plt.text(3000, 840, "max length in train set's non-spam: " + str(max(train_set[train_set['label']==0]['length'])), fontsize=20, color='b')
plt.text(3000, 800, "avg length in train set's non-spam:"+ str(sum(train_set[train_set['label']==0]['length']) / len(train_set[train_set['label']==0]['length'])), fontsize=20, color='b')


plt.xlabel('legnth')
plt.ylabel('mail')
plt.legend(['total mail', 'spam', 'non-spam'])
plt.show()

def word_count_plot(mail, title):     
     word_counter = collections.Counter([word for sentence in mail for word in sentence.split()])
     most_count = word_counter.most_common(50)
     most_count = pd.DataFrame(most_count, columns=["Word", "Frequent"]).sort_values(by="Frequent")
     most_count.plot.barh(title=title, x = "Word", y = "Frequent", figsize=(6, 12))
     # ë©”ì¼ ì¤‘ ê°€ì¥ í”í•œ ë‹¨ì–´ ìƒìœ„ 50ê°œ ì‹œê°í™”

word_count_plot(train_set['mail'], "Mails in train_set")

word_count_plot(train_set[train_set['label']==0].mail, "Normal mails in train_set")
word_count_plot(train_set[train_set['label']==1].mail, "Spam mails in train_set")

"""ì „ë°˜ì ìœ¼ë¡œ ì˜ë¯¸ë¥¼ ì•Œê¸° ì–´ë µê±°ë‚˜('.', ','), ë¶ˆìš©ì–´(a, to), íŠ¹ìˆ˜ë¬¸ì(@, $)ê°€ ì¡´ì¬í•œë‹¤. 

ë°ì´í„° í´ë¦¬ë‹ ì´í›„ì—ëŠ” ì–´ë–¤ì§€ ë‹¤ì‹œ í™•ì¸í•´ë³´ì.

ML ë° DL í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„°ë¥¼ ê°€ê³µí•˜ëŠ” ì¡°ê±´ì„ ë‹¤ì–‘í•˜ê²Œ ì„¤ì •í•´ í™•ì¸í•´ë³¸ë‹¤.

# 2. ë°ì´í„° ì „ì²˜ë¦¬
> ë°ì´í„° í´ë¦¬ë‹

ë‹¨ì–´ë¥¼ ê° ê¸°ì¤€(ì •ê·œí™”, í‘œì œì–´, ë¶ˆìš©ì–´ ë“±)ì— ë”°ë¼ ê°€ê³µí•œë‹¤. ë‹¤ì–‘í•œ ê°€ê³µ ì¡°ê±´ ì¤‘ íŠ¹íˆ ë„¤ ê°€ì§€ë¥¼ ë½‘ì•„ ì´í›„ ëª¨ë¸ì— ì ìš©í•œë‹¤.
"""

lem = WordNetLemmatizer()
def preprocess(mail, contract, lower, noops, stop, lemma):  
      text = mail
      if contract: text = contractions.fix(mail) # ì•½ì–´ ì •ê·œí™” (i'm -> i am)
      if lower: text = text.lower() # ì†Œë¬¸ìí™”
      if noops: text = re.sub("[^a-z ]", "", text) # íŠ¹ìˆ˜ë¬¸ì, ìˆ«ì ì œê±°
      text = text.split()
      if stop: text = [word for word in text if not word in set(stopwords.words('english'))] # ë¶ˆìš©ì–´ ì œê±°
      if lemma: text = [lem.lemmatize(word) for word in text] # í‘œì œì–´
      text = " ".join(text)
      return text

train_input1 = train_set['mail'].apply(lambda x: preprocess(x, contract=True, lower=True, noops=True, stop=True, lemma=True))
test_input1 = test_set['mail'].apply(lambda x: preprocess(x, contract=True, lower=True, noops=True, stop=True, lemma=True))

"""1ë²ˆ: ì£¼ì–´ì§„ ì¡°ê±´ì„ ëª¨ë‘ ì‚¬ìš©í•˜ì˜€ë‹¤."""

word_count_plot(train_input1, "Mails in train_input1")

train_input2 = train_set['mail'].apply(lambda x: preprocess(x, contract=True, lower=False, noops=True, stop=True, lemma=True))
test_input2 = test_set['mail'].apply(lambda x: preprocess(x, contract=True, lower=False, noops=True, stop=True, lemma=True))

"""2ë²ˆ: ì†Œë¬¸ìí™”ë§Œ í•˜ì§€ ì•Šì•˜ë‹¤."""

word_count_plot(train_input2, "Mails in train_input2")

train_input3 = train_set['mail'].apply(lambda x: preprocess(x, contract=True, lower=True, noops=False, stop=True, lemma=True))
test_input3 = test_set['mail'].apply(lambda x: preprocess(x, contract=True, lower=True, noops=False, stop=True, lemma=True))

"""3ë²ˆ: íŠ¹ìˆ˜ë¬¸ì ë° ìˆ«ìë¥¼ ì œê±°í•˜ì§€ ì•Šì•˜ë‹¤.

"""

word_count_plot(train_input3, "Mails in train_input3")

train_input4 = train_set['mail'].apply(lambda x: preprocess(x, contract=True, lower=True, noops=True, stop=False, lemma=False))
test_input4 = test_set['mail'].apply(lambda x: preprocess(x, contract=True, lower=True, noops=True, stop=False, lemma=False))

"""4ë²ˆ: ë¶ˆìš©ì–´ì™€ í‘œì œì–´ë¥¼ ì œê±°í•˜ì§€ ì•Šì•˜ë‹¤."""

word_count_plot(train_input4, "Mails in train_input4")

"""

> Countvectorizer, TF-IDF

ê°€ê³µí•œ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°í™”í•œë‹¤. ë‹¨ì–´ ë¹ˆë„ì— ë”°ë¼ BoW(Bag of Words)ë¥¼ ë§Œë“œëŠ” Countvectorizerì™€ ê°€ì¤‘ì¹˜ë¥¼ ë‘” TF-IDFë¥¼ ì‚¬ìš©í•œë‹¤. 

ë²¡í„°í™”í•œ ë°ì´í„°ëŠ” ì´í›„ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ(multinomialNB), ë¡œì§€ìŠ¤í‹± íšŒê·€ ë¶„ë¥˜ ê¸°ë²•ì— ì‚¬ìš©í•œë‹¤."""

def vectorizer(train_input, val_input, test_input):
    vectorizer = CountVectorizer()
    train_input = vectorizer.fit_transform(train_input)
    val_input = vectorizer.transform(val_input)
    test_input = vectorizer.transform(test_input)
    return (train_input, val_input, test_input)

def tfidv(train_input, val_input, test_input):
    tfidv = TfidfVectorizer()
    train_input = tfidv.fit_transform(train_input)
    val_input = tfidv.transform(val_input)
    test_input = tfidv.transform(test_input)
    return (train_input, val_input, test_input)

"""> Tokenizer

ê°€ê³µí•œ ë°ì´í„°ë¥¼ í† í°í™”(í…ìŠ¤íŠ¸â†’ì‹œí€¸ìŠ¤) ë° íŒ¨ë”©í•œë‹¤.
ë²¡í„°í™”í•œ ë°ì´í„°ëŠ” ì´í›„ LSTM ëª¨ë¸ì— ì‚¬ìš©í•œë‹¤.
"""

def tokenizer(train_input, val_input, test_input, max_len):
    tokenizer = Tokenizer(num_words= max_len)
    tokenizer.fit_on_texts(train_input)
    train_sequence = tokenizer.texts_to_sequences(train_input)
    val_sequence = tokenizer.texts_to_sequences(val_input)
    test_sequence = tokenizer.texts_to_sequences(test_input)
    max_words = len(tokenizer.word_index) + 1
    return (train_sequence, val_sequence, test_sequence, max_words)

def padding(train_sequence, val_sequence, test_sequence, max_length_sequence):
    train_padded = pad_sequences(train_sequence, maxlen=max_length_sequence)
    val_padded = pad_sequences(val_sequence, maxlen=max_length_sequence)
    test_padded = pad_sequences(test_sequence, maxlen=max_length_sequence)
    return (train_padded, val_padded, test_padded)

train_target = train_set['label']

train_input1, val_input1, train_target1, val_target1 = train_test_split(train_input1, train_target, test_size=0.2, stratify=train_target)
train_input2, val_input2, train_target2, val_target2 = train_test_split(train_input2, train_target, test_size=0.2, stratify=train_target)
train_input3, val_input3, train_target3, val_target3 = train_test_split(train_input3, train_target, test_size=0.2, stratify=train_target)
train_input4, val_input4, train_target4, val_target4 = train_test_split(train_input4, train_target, test_size=0.2, stratify=train_target)

"""ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ í†µí•´ ê°€ê³µí•œ ë°ì´í„°ë¥¼ train_input, val_inputìœ¼ë¡œ ë¶„ë¥˜í•œë‹¤. 

ì›ë³¸ ë°ì´í„° ë‚´ ìŠ¤íŒ¸ ë¹„ìœ¨ì´ ë” ì ìœ¼ë¯€ë¡œ, stratifyë¥¼ í†µí•´ labelì´ ì¹˜ìš°ì³ì§€ì§€ ì•Šë„ë¡ í•œë‹¤.
"""

train_vect1, val_vect1, test_vect1 = vectorizer(train_input1, val_input1, test_input1)
train_vect2, val_vect2, test_vect2 = vectorizer(train_input2, val_input2, test_input2)
train_vect3, val_vect3, test_vect3 = vectorizer(train_input3, val_input3, test_input3)
train_vect4, val_vect4, test_vect4 = vectorizer(train_input4, val_input4, test_input4)


train_tf1, val_tf1, test_tf1 = tfidv(train_input1, val_input1, test_input1)
train_tf2, val_tf2, test_tf2 = tfidv(train_input2, val_input2, test_input2)
train_tf3, val_tf3, test_tf3 = tfidv(train_input3, val_input3, test_input3)
train_tf4, val_tf4, test_tf4 = tfidv(train_input4, val_input4, test_input4)

"""train_input, val_input, test_inputì„ ê°ê° ë²¡í„°í™”í•œë‹¤."""

max_len = 1000
train_sequence1, val_sequence1, test_sequence1, word_index1 = tokenizer(train_input1, val_input1, test_input1, max_len)
max_length_sequence1 = max([len(i) for i in train_sequence1])
train_pad1, val_pad1, test_pad1 = padding(train_sequence1, val_sequence1, test_sequence1, max_length_sequence1)

train_sequence2, val_sequence2, test_sequence2, word_index2 = tokenizer(train_input2, val_input2, test_input2, max_len)
max_length_sequence2 = max([len(i) for i in train_sequence2])
train_pad2, val_pad2, test_pad2 = padding(train_sequence2, val_sequence2, test_sequence2, max_length_sequence2)

train_sequence3, val_sequence3, test_sequence3, word_index3 = tokenizer(train_input3, val_input3, test_input3, max_len)
max_length_sequence3 = max([len(i) for i in train_sequence3])
train_pad3, val_pad3, test_pad3 = padding(train_sequence3, val_sequence3, test_sequence3, max_length_sequence3)

train_sequence4, val_sequence4, test_sequence4, word_index4 = tokenizer(train_input4, val_input4, test_input4, max_len)
max_length_sequence4 = max([len(i) for i in train_sequence4])
train_pad4, val_pad4, test_pad4 = padding(train_sequence4, val_sequence4, test_sequence4, max_length_sequence4)

"""ë§ˆì°¬ê°€ì§€ë¡œ í† í°í™”í•œ ë’¤ ê°€ì¥ ê¸´ ê¸¸ì´ì— ë§ì¶° íŒ¨ë”©í•œë‹¤.

# 3. ML ëª¨ë¸ ë° ì„±ëŠ¥ í‰ê°€

> MultinomialNB

ë‚˜ì´ë¸Œ ë² ì´ì¦ˆì˜ ëŒ€í‘œì ì¸ ê¸°ë²• MultinomialNBì— ë²¡í„°í™”í•œ ë°ì´í„°ë¥¼ ì ìš©í•œë‹¤.

í›ˆë ¨ì´ ëë‚œ ë’¤ val_input, val_targetì„ í†µí•´ í›ˆë ¨ì´ ì˜ ë˜ì—ˆëŠ”ì§€ ì ê²€í•œë‹¤. 

ì˜ˆì¸¡í•œ ìë£Œ(val_pred)ë¥¼ ì‹¤ì œ ê°’(val_target)ì™€ ë¹„êµí•´ confusion matrix, classification reportë¥¼ ì‘ì„±, ì„±ëŠ¥ì„ í‰ê°€í•  ìˆ˜ ìˆë‹¤.
"""

def report(val_target, val_pred, title):
    cm = pd.DataFrame(confusion_matrix(val_target, val_pred))
    sns.heatmap(cm, annot = True, fmt = 'd',cmap = 'Reds')
    plt.title(title)
    plt.show()

    print("Classification Report: \n", classification_report(val_target, val_pred))

def multiNB(train_input, train_target, val_input, val_target, test_input):
    nb = MultinomialNB()
    nb.fit(train_input, train_target)
    val_pred = nb.predict(val_input)

    test_pred = nb.predict(test_input)
    return (val_pred, test_pred)

val_pred1_vect_nb, test_pred1_vect_nb = multiNB(train_vect1, train_target1, val_vect1, val_target1, test_vect1)
report(val_target1, val_pred1_vect_nb, "MultinomialNB's confusion matrix: val prediction, CountVectorizer, train_input1")

val_pred2_vect_nb, test_pred2_vect_nb = multiNB(train_vect2, train_target2, val_vect2, val_target2, test_vect2)
report(val_target2, val_pred2_vect_nb, "MultinomialNB's confusion matrix: val prediction, CountVectorizer, train_input2")

val_pred3_vect_nb, test_pred3_vect_nb = multiNB(train_vect3, train_target3, val_vect3, val_target3, test_vect3)
report(val_target3, val_pred3_vect_nb, "MultinomialNB's confusion matrix: val prediction, CountVectorizer, train_input3")

val_pred4_vect_nb, test_pred4_vect_nb = multiNB(train_vect4, train_target4, val_vect4, val_target4, test_vect4)
report(val_target4, val_pred4_vect_nb, "MultinomialNB's confusion matrix: val prediction, CountVectorizer, train_input4")

val_pred1_tf_nb, test_pred1_tf_nb = multiNB(train_tf1, train_target1, val_tf1, val_target1, test_tf1)
report(val_target1, val_pred1_tf_nb, "MultinomialNB's confusion matrix: val prediction, TF-ID Vectorizer, train_input1")

val_pred2_tf_nb, test_pred2_tf_nb = multiNB(train_tf2, train_target2, val_tf2, val_target2, test_tf2)
report(val_target2, val_pred2_tf_nb, "MultinomialNB's confusion matrix: val prediction, TF-ID Vectorizer, train_input2")

val_pred3_tf_nb, test_pred3_tf_nb = multiNB(train_tf3, train_target3, val_tf3, val_target3, test_tf3)
report(val_target3, val_pred3_tf_nb, "MultinomialNB's confusion matrix: val prediction, TF-ID Vectorizer, train_input3")

val_pred4_tf_nb, test_pred4_tf_nb = multiNB(train_tf4, train_target4, val_tf4, val_target4, test_tf4)
report(val_target4, val_pred4_tf_nb, "MultinomialNB's confusion matrix: val prediction, TF-ID Vectorizer, train_input4")

"""> Logistic Regression

ë¡œì§€ìŠ¤í‹± íšŒê·€ ë¶„ë¥˜ ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤.

ë‚˜ì´ë¸Œ ë² ì´ì¦ˆì™€ ë§ˆì°¬ê°€ì§€ë¡œ í›ˆë ¨ì´ ëë‚œ ë’¤ val_input, val_targetì„ í†µí•´ ì„±ëŠ¥ í‰ê°€ ì§€í‘œë¥¼ ë³´ì—¬ì¤€ë‹¤.
"""

def lr(train_input, train_target, val_input, val_target, test_input):
    lr = LogisticRegression()
    lr.fit(train_input, train_target)
    val_pred = lr.predict(val_input)
    test_pred = lr.predict(test_input)
    return val_pred, test_pred

val_pred1_vect_lr, test_pred1_vect_lr = lr(train_vect1, train_target1, val_vect1, val_target1, test_vect1)
report(val_target1, val_pred1_vect_lr, "Logistic Regression's confusion matrix: val prediction, CountVectorizer, train_input1")

val_pred2_vect_lr, test_pred2_vect_lr = lr(train_vect2, train_target2, val_vect2, val_target2, test_vect2)
report(val_target2, val_pred2_vect_lr, "Logistic Regression's confusion matrix: val prediction, CountVectorizer, train_input2")

val_pred3_vect_lr, test_pred3_vect_lr = lr(train_vect3, train_target3, val_vect3, val_target3, test_vect3)
report(val_target3, val_pred3_vect_lr, "Logistic Regression's confusion matrix: val prediction, CountVectorizer, train_input3")

val_pred4_vect_lr, test_pred4_vect_lr = lr(train_vect4, train_target4, val_vect4, val_target4, test_vect4)
report(val_target4, val_pred4_vect_lr, "Logistic Regression's confusion matrix: val prediction, CountVectorizer, train_input4")

val_pred1_tf_lr, test_pred1_tf_lr = lr(train_tf1, train_target1, val_tf1, val_target1, test_tf1)
report(val_target1, val_pred1_tf_lr, "Logistic Regression's confusion matrix: val prediction, TF-ID Vectorizer, train_input1")

val_pred2_tf_lr, test_pred2_tf_lr = lr(train_tf2, train_target2, val_tf2, val_target2, test_tf2)
report(val_target2, val_pred2_tf_lr, "Logistic Regression's confusion matrix: val prediction, TF-ID Vectorizer, train_input2")

val_pred3_tf_lr, test_pred3_tf_lr = lr(train_tf3, train_target3, val_tf3, val_target3, test_tf3)
report(val_target3, val_pred3_tf_lr, "Logistic Regression's confusion matrix: val prediction, TF-ID Vectorizer, train_input3")

val_pred4_tf_lr, test_pred4_tf_lr = lr(train_tf4, train_target4, val_tf4, val_target4, test_tf4)
report(val_target4, val_pred4_tf_lr, "Logistic Regression's confusion matrix: val prediction, TF-ID Vectorizer, train_input4")

def create_model(max_words, max_length_sequence):
    
      lstm_model = Sequential()
      lstm_model.add(Embedding(max_words, 50, input_length=max_length_sequence))
      lstm_model.add(LSTM(64))
      lstm_model.add(Dropout(0.3))
      lstm_model.add(Dense(20, activation="relu"))
      lstm_model.add(Dropout(0.3))
      lstm_model.add(Dense(1, activation = "sigmoid"))
      lstm_model.compile(loss = "binary_crossentropy", optimizer = "adam", metrics = ["accuracy"])
      lstm_model.summary()
      return lstm_model

def lstm_run(lstm_model, train_padded, train_target, val_padded, val_target, test_padded):
    checkpoint_cb = keras.callbacks.ModelCheckpoint('best-lstm-model.h5')
    early_stopping_cb = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)
    history = lstm_model.fit(train_padded, train_target, epochs=100, batch_size=64, validation_data=(val_padded, val_target), callbacks=[checkpoint_cb, early_stopping_cb])
    
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.legend(['train', 'val'])
    plt.show()

    return ((lstm_model.predict(val_padded) > 0.5).astype("int32"), (lstm_model.predict(test_padded) > 0.5).astype("int32"))

"""> LSTM
ë”¥ëŸ¬ë‹ ëª¨ë¸ ì¤‘ LSTM ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤.

relu, sigmoid í•¨ìˆ˜ë¥¼ ì ìš©í•œ Sequential í•¨ìˆ˜ë¡œ ì¤‘ê°„ Dropoutì„ ë‘ ë²ˆ ì ìš©í•˜ì˜€ë‹¤.

optimizerë¡œ adamì„ ì‚¬ìš©í–ˆê³ , validation lossê°€ ë‘ ë²ˆ ì—°ì† ê°ì†Œí•˜ì§€ ì•Šì„ ê²½ìš° í›ˆë ¨ì„ ì¢…ë£Œí•œë‹¤. 

historyì— ì €ì¥í•´ë†“ì€ train_loss, val_lossë¥¼ ë„ì‹í™”í•˜ì—¬ í‘œí˜„í•œë‹¤.

ë§ˆì°¬ê°€ì§€ë¡œ LSTM ëª¨ë¸ì´ ì˜ˆì¸¡í•œ val_predì™€ val_targetì„ ë¹„êµí•´ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.
 
ì´ë•Œ predict_classê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ 0.5ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¼ë²¨ë§í•˜ì˜€ë‹¤.
"""

lstm_model1 = create_model(word_index1, max_length_sequence1)
val_pred1_lstm, test_pred1_lstm = lstm_run(lstm_model1, train_pad1, train_target1, val_pad1, val_target1, test_pad1)

report(val_target1, val_pred1_lstm, "LSTM's confusion matrix: val prediction, train_input1")

lstm_model2 = create_model(word_index2, max_length_sequence2)
val_pred2_lstm, test_pred2_lstm = lstm_run(lstm_model2, train_pad2, train_target2, val_pad2, val_target2, test_pad2)

report(val_target2, val_pred2_lstm, "LSTM's confusion matrix: val prediction, train_input2")

lstm_model3 = create_model(word_index3, max_length_sequence3)
val_pred3_lstm, test_pred3_lstm = lstm_run(lstm_model3, train_pad3, train_target3, val_pad3, val_target3, test_pad3)

report(val_target3, val_pred3_lstm, "LSTM's confusion matrix: val prediction, train_input3")

lstm_model4 = create_model(word_index4, max_length_sequence4)
val_pred4_lstm, test_pred4_lstm = lstm_run(lstm_model4, train_pad4, train_target4, val_pad4, val_target4, test_pad4)

report(val_target4, val_pred4_lstm, "LSTM's confusion matrix: val prediction, train_input4")

"""# 4. ìŠ¤íŒ¸ í•„í„°ë§ ëª¨ë¸

> ëª¨ë¸ ì„ ì •: ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë° LSTM

1). ML: CountVectorizer, TF-ID Vectorizerì— ê¸°ë°˜í•œ MultinomialNB, LogistricRegrssion 2). DL: Tokenizer-Paddingì„ í†µí•œ LSTMì˜ ì„±ëŠ¥ í‰ê°€ë¥¼ í†µí•´, í•„í„°ë§í•  ì¡°ê±´ì„ ê³ ë¥¸ë‹¤.

TF-ID Vectorizerì— ê¸°ë°˜í•œ MultinomialNBê°€ ì¼ë°˜ ë©”ì¼ë¡œ ë¶„ë¥˜í–ˆë‹¤ë©´ ë„˜ì–´ê°€ê³ , ìŠ¤íŒ¸ìœ¼ë¡œ ë¶„ë¥˜í–ˆë‹¤ë©´ ì´ì°¨ì ìœ¼ë¡œ LSTM ëª¨ë¸ì„ í†µí•´ ë‹¤ì‹œ í•œ ë²ˆ ê²€ì‚¬í•œë‹¤. 

ì´ë•Œ ê³ ë ¤í•´ì•¼ í•  ì¡°ê±´ì€ ì „ì²˜ë¦¬í•œ ë°ì´í„°ì˜ ì¢…ë¥˜, MLì˜ ì¢…ë¥˜, MLì´ íƒí•˜ëŠ” ë²¡í„°í™” ê¸°ë²•ì˜ ì¢…ë¥˜ ë“±ì´ ì¡´ì¬í•œë‹¤.


"""

nb_lstm_pred = np.array([])

for i in range(len(test_pred4_tf_nb)):
    if test_pred4_tf_nb[i] == 0:
        nb_lstm_pred = np.append(nb_lstm_pred, 0)
    else:
        nb_lstm_pred = np.append(nb_lstm_pred, test_pred4_lstm[i])

"""ë„¤ ë²ˆì§¸ ì „ì²˜ë¦¬ ë°ì´í„°(ë‹¨ì–´ ì •ê·œí™”, ì†Œë¬¸ìí™”, íŠ¹ìˆ˜ë¬¸ì ë° ìˆ«ì ì œì™¸)ë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤. ì¦‰ ë¶ˆìš©ì–´ë¥¼ ê±¸ëŸ¬ë‚´ì§€ ì•Šì€ ë°ì´í„°ì´ë‹¤.

ì´ ë°ì´í„°ë¥¼ TF-ID Vectorizerë¥¼ í†µí•´ ê°€ì¤‘ì¹˜ì— ë”°ë¼ ë²¡í„°í™”í•˜ê³ , ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ multinomialNBë¥¼ í†µí•´ 1ì°¨ ë¼ë²¨ë§ì„ ê²€ì‚¬í•œë‹¤. ë§Œì¼ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆê°€ ì •ìƒ ë©”ì¼ì´ë¼ ê°„ì£¼í•œë‹¤ë©´ ì •ìƒ ë©”ì¼ë¡œ ê°„ì£¼í•œë‹¤. ìŠ¤íŒ¸ ë©”ì¼ë¡œ ê°„ì£¼í•œ ê²½ìš° LSTM ëª¨ë¸ì´ 2ì°¨ ë¼ë²¨ë§ì„ ê²€ì‚¬í•œë‹¤.

> í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡

ëª¨ë¸ ì„ ì • ì´í›„ test_input4ë¥¼ ì˜ˆì¸¡í•œ ê°’ì„ csv íŒŒì¼ë¡œ ì €ì¥í•œë‹¤.
"""

result = pd.concat([test_set['id'], pd.DataFrame(nb_lstm_pred).astype(int)], axis=1, ignore_index=True)

result = result.rename(columns={0:'id', 1:'label'})

result.to_csv('result_data.csv', index=False)

